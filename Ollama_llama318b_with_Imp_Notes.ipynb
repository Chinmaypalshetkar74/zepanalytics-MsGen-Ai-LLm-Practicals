{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09788819-775f-473b-8354-89b1e183ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec11bb-9aad-4a1b-9d44-339b5ff4a78e",
   "metadata": {},
   "source": [
    "- from langchain_community.llms import Ollama:\n",
    "Imports the community version of Ollama LLM wrapper for LangChain. May be used in older setups or custom workflows.\n",
    "- from langchain_ollama import OllamaLLM:\n",
    "Imports the official Ollama integration for LangChain. Preferred for current, stable use with local models like LLaMA, Mistral, etc.\n",
    "Use one of them depending on your setup. For new projects, go with OllamaLLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcf69bb-da88-4324-9fa5-96e3f8768c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weare\\AppData\\Local\\Temp\\ipykernel_16784\\2378445012.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model='llama3.1:8b')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model='llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679314d-2b24-4543-afb6-12f631b80bd0",
   "metadata": {},
   "source": [
    "- llm = Ollama(model='llama3.1:8b'):\n",
    "This initializes a local language model using Ollama.\n",
    "It loads the model named 'llama3.1:8b', which likely refers to LLaMA 3.1 with 8 billion parameters.\n",
    "‚úÖ After this, llm can be used to generate text or interact with the model in LangChain workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9c11fc-e3a4-4fa0-896c-a2f9302ff4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some of the most popular actors in India:\n",
      "\n",
      "**Male Actors:**\n",
      "\n",
      "1. **Shah Rukh Khan**: Known as the \"King of Bollywood\", he has acted in numerous hit films like Dilwale Dulhania Le Jayenge, Kuch Kuch Hota Hai, and Kabhi Khushi Kabhie Gham.\n",
      "2. **Amitabh Bachchan**: A legendary actor who has been active in the industry for over 50 years, known for his iconic roles in films like Sholay, Deewar, and Black.\n",
      "3. **Salman Khan**: One of the highest-paid actors in India, known for his blockbuster hits like Bajrangi Bhaijaan, Sultan, and Dabangg.\n",
      "4. **Hrithik Roshan**: A versatile actor known for his energetic performances in films like Kaho Naa Pyaar Hai, Dhoom 2, and Kaabil.\n",
      "5. **Ranveer Singh**: Known for his energetic and flamboyant performances in films like Bajirao Mastani, Padmaavat, and Gully Boy.\n",
      "\n",
      "**Female Actors:**\n",
      "\n",
      "1. **Priyanka Chopra**: A popular actress who has acted in numerous hit films like Kaminey, Barfi!, and Mary Kom.\n",
      "2. **Deepika Padukone**: One of the highest-paid actresses in India, known for her performances in films like Om Shanti Om, Cocktail, and Piku.\n",
      "3. **Alia Bhatt**: A talented young actress who has quickly risen to fame with hits like Highway, 2 States, and Gully Boy.\n",
      "4. **Kareena Kapoor Khan**: Known for her iconic roles in films like Jab We Met, 3 Idiots, and Veere Di Wedding.\n",
      "5. **Katrina Kaif**: A popular actress known for her performances in films like Bharat, Ek Tha Tiger, and Zero.\n",
      "\n",
      "These are just a few examples of the many talented actors working in India's film industry today!\n"
     ]
    }
   ],
   "source": [
    "question = \"Popular actor of india\"\n",
    "response = llm.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a06842f-ff6a-4a33-8579-9b9232004632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suniel Shetty is an Indian actor, film producer, and television personality who has been active in the Hindi film industry since the late 1980s. He was born on August 19, 1961, in Mulki, Karnataka, India.\n",
      "\n",
      "Shetty began his acting career with a small role in the 1988 film \"Balwan\", but it was his breakthrough role as a villain in the 1992 film \"Dil\" that brought him to prominence. He then went on to play lead roles in several successful films, including \"Hum Hain Khalnayak\" (1994), \"Gopi Kishan\" (1994), and \"Aaditya\" (1995).\n",
      "\n",
      "Shetty's most notable role was perhaps as the villainous Rakka in Rajiv Mehra's 1993 film \"Dilwale Dulhania Le Jayenge\", which is one of the highest-grossing films of all time in Indian cinema.\n",
      "\n",
      "In addition to his acting career, Shetty has also ventured into production with his company, Sunshine Productions. He has produced several films, including \"Gadar: Ek Prem Katha\" (2001), \"Krishna Cottage\" (2006), and \"Tera Mera Ki Rishta\" (2010).\n",
      "\n",
      "Shetty has also made appearances on television shows, such as the popular dance reality show \"Jhalak Dikhhla Jaa\" in 2014. He is known for his charismatic personality and has been involved in various social causes, including animal welfare and supporting underprivileged children.\n",
      "\n",
      "Throughout his career, Shetty has received several awards and nominations, including a Filmfare Award nomination for Best Actor in a Negative Role for \"Dilwale Dulhania Le Jayenge\".\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"Who is sunil shetty \"\n",
    "response = llm.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73a75c-5605-4f39-b0d3-65d65eb53d5a",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing RAG for custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d6151-1e7b-4d51-8c91-f897d9fb3cf5",
   "metadata": {},
   "source": [
    "- PyPDFLoader: Loads text content from a PDF file and converts it into LangChain Document objects.\n",
    "- RecursiveCharacterTextSplitter: Splits large text into smaller chunks (e.g., 500‚Äì1000 characters) while preserving sentence structure as much as possible. Useful for feeding into LLMs that have token limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ea5c8f-4ac3-4865-9ccf-2fbd731fcfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdc23834-a861-433f-8348-d52829334edd",
   "metadata": {},
   "source": [
    "\n",
    "pdf_loader = PyPDFLoader(\"Attention Is All You Need.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1100,\n",
    "    chunk_overlap=140\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Total Chunks:\", len(chunks))\n",
    "print(chunks[0].page_content[:52])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1d370-fe31-41b3-bad5-7445b97fc3c8",
   "metadata": {},
   "source": [
    "- Loads the full PDF into a list of **Document** objects.\n",
    "- Each **Document** contains text and metadata (like page number).\n",
    "- Breaks the full text into smaller overlapping chunks.\n",
    "- **chunk_size=1100**: Each chunk has ~1100 characters.\n",
    "- **chunk_overlap=140**: Each chunk overlaps the previous by 140 characters to preserve context.\n",
    "- Prints how many chunks were created.\n",
    "- Displays the first 800 characters of the first chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fd5f4-8c8c-433d-ac12-93c73a337cc6",
   "metadata": {},
   "source": [
    "# Notes -\n",
    "\n",
    "### ‚úÖ How to Decide Chunk Size Based on PDF Pages (Correct Method)\n",
    "\n",
    "- You decide parameters using 3 factors:\n",
    "- 1Ô∏è‚É£ Step 1 ‚Äî Understand PDF Type\n",
    "| PDF Type           | Examples                | Best Chunk Size |\n",
    "| ------------------ | ----------------------- | --------------- |\n",
    "| Research Paper     | Transformers, ML papers | **1500‚Äì2000**   |\n",
    "| Textbooks          | ML, DS books            | **1800‚Äì2500**   |\n",
    "| Code/Documentation | LangChain, APIs         | **800‚Äì1200**    |\n",
    "| Stories/Novels     | Fiction, articles       | **1000‚Äì1500**   |\n",
    "| Legal/Contracts    | Agreements, policies    | **1200‚Äì1800**   |\n",
    "\n",
    "- 2Ô∏è‚É£ Step 2 ‚Äî Estimate Text Density (Important)\n",
    "\n",
    "The number of pages alone is meaningless because:\n",
    "\n",
    "A 50-page paper = ~20,000 words\n",
    "\n",
    "A 50-page textbook = ~40,000‚Äì60,000 words\n",
    "\n",
    "A 50-page presentation-style PDF = ~4,000 words\n",
    "\n",
    "So instead, ask:\n",
    "\n",
    "Does 1 page contain heavy text?\n",
    "\n",
    "If yes, use larger chunks.\n",
    "- Simple rule:\n",
    "| Page Text Density | Signs                      | Chunk Size    |\n",
    "| ----------------- | -------------------------- | ------------- |\n",
    "| High Density      | Full paragraphs, equations | **1500‚Äì2000** |\n",
    "| Medium            | Normal text                | **1200‚Äì1500** |\n",
    "| Low               | Bullet slides              | **600‚Äì900**   |\n",
    "\n",
    "- 3Ô∏è‚É£ Step 3 ‚Äî Simple Formula for Chunk Size Based on Pages\n",
    "\n",
    "If you still want a formula, here is the best practical one:\n",
    "Formula (ML research/general books):\n",
    "chunk_size = 35000 / number_of_chunks_you_want\n",
    "\n",
    "\n",
    "To get good RAG performance:\n",
    "\n",
    "Best practice:\n",
    "Aim for 50‚Äì80 chunks total.\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "For a 52-page research paper ‚Üí Target 60‚Äì70 chunks\n",
    "\n",
    "Use:\n",
    "\n",
    "chunk_size = (total_characters / 60)\n",
    "\n",
    "\n",
    "But you don't know characters ‚Üí so use this shortcut:\n",
    "\n",
    "Shortcut Rule\n",
    "chunk_size = 1500 + (pages / 10 * 50)\n",
    "\n",
    "\n",
    "For 52 pages:\n",
    "\n",
    "chunk_size = 1500 + (52/10 * 50)\n",
    "            = 1500 + 260\n",
    "            = 1760\n",
    "\n",
    "\n",
    "Perfect for research papers.\n",
    "\n",
    "üéØ Final Decision Shortcut (use this always)\n",
    "üìò If PDF has dense text (research/math/books):\n",
    "\n",
    "üëâ chunk_size = 1600‚Äì2000\n",
    "üëâ chunk_overlap = 200‚Äì300\n",
    "\n",
    "üìÑ If PDF has normal paragraphs:\n",
    "\n",
    "üëâ chunk_size = 1200‚Äì1500\n",
    "üëâ chunk_overlap = 150‚Äì200\n",
    "\n",
    "üñ•Ô∏è If PDF has slides (PPT-like):\n",
    "\n",
    "üëâ chunk_size = 600‚Äì800\n",
    "üëâ chunk_overlap = 100‚Äì150\n",
    "\n",
    "\n",
    "### ‚≠ê Super Simple Guide\n",
    "| Pages   | Type           | Recommended Chunk Size |\n",
    "| ------- | -------------- | ---------------------- |\n",
    "| 20‚Äì50   | Research paper | **1500‚Äì1800**          |\n",
    "| 50‚Äì150  | Textbook       | **1800‚Äì2200**          |\n",
    "| 150‚Äì500 | Long books     | **2000‚Äì2500**          |\n",
    "| 10‚Äì30   | Slides         | **600‚Äì900**            |\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da763b22-7a20-4a97-bfd4-a9b7f4d21915",
   "metadata": {},
   "source": [
    "#### üî• Want automatic chunk size selection?\n",
    "\n",
    "##### here is a Python function:\n",
    "\n",
    "def auto_chunk_size(pages, density=\"high\"):\n",
    "    if density == \"high\":   # Research papers, textbooks\n",
    "        return min(2000, 1500 + pages * 5)\n",
    "    elif density == \"medium\":  # Normal text PDFs\n",
    "        return min(1800, 1200 + pages * 3)\n",
    "    else:  # Slides / bullet-style\n",
    "        return 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fa3686-1ceb-4914-9437-4e6c264d86fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 39\n",
      "Provided proper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_loader = PyPDFLoader(\"Attention Is All You Need.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1300,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Total Chunks:\", len(chunks))\n",
    "print(chunks[0].page_content[:15])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fe20af-dc9d-439d-94a3-5353bb219265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weare\\AppData\\Local\\Temp\\ipykernel_16784\\3767687740.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Build FAISS vector store\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0e2fd-5dfd-4ae4-b576-486b29b41cdb",
   "metadata": {},
   "source": [
    "- **HuggingFaceEmbeddings**: Loads a sentence transformer model to convert text into numerical vectors (embeddings).\n",
    "- **FAISS**: A fast vector similarity search library used to store and search embeddings efficiently\n",
    "\n",
    "\n",
    "- Loads the **all-mpnet-base-v2 model** from Hugging Face.\n",
    "- This model turns each text chunk into a dense vector that captures its meaning.\n",
    "- Converts all **chunks** into vectors using the embedding model.\n",
    "- Stores them in a FAISS index for fast similarity search.\n",
    "- Converts the **FAISS index** into a retriever object.\n",
    "- You can now use **retriever.get_relevant_documents(query)** to fetch chunks similar to a user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29df6e3-8e44-4cef-80e6-ad1dda62c344",
   "metadata": {},
   "source": [
    "# Notes -\n",
    "#### ‚úÖ Recommended Embedding Models for RAG\n",
    "1Ô∏è‚É£ all-mpnet-base-v2 (your current choice)\n",
    "\n",
    "Model: \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "Pros:\n",
    "\n",
    "Very strong semantic understanding\n",
    "\n",
    "Excellent for short & long texts\n",
    "\n",
    "High-quality embeddings for English research papers\n",
    "\n",
    "Cons:\n",
    "\n",
    "Slightly slower than smaller models\n",
    "\n",
    "Verdict: ‚úÖ Excellent choice for research papers\n",
    "\n",
    "### 2Ô∏è‚É£ Other HuggingFace Sentence Transformers\n",
    "| Model                       | Strengths                             | Use Case                                                             |\n",
    "| --------------------------- | ------------------------------------- | -------------------------------------------------------------------- |\n",
    "| `all-MiniLM-L6-v2`          | Lightweight, fast, smaller embeddings | If you want **faster retrieval** and can sacrifice a little accuracy |\n",
    "| `all-mpnet-base-v2`         | High accuracy                         | Best **balance for research papers**                                 |\n",
    "| `multi-qa-MiniLM-L6-cos-v1` | Optimized for **question-answering**  | If RAG is mostly **QA-based**                                        |\n",
    "| `all-mpnet-base-v1`         | Slightly older version                | Slightly less accurate, cheaper                                      |\n",
    "\n",
    "3Ô∏è‚É£ For Large Docs / Dense Research Papers\n",
    "\n",
    "You can use multi-qa-mpnet-base-dot-v1 or all-mpnet-base-v2\n",
    "\n",
    "Why?\n",
    "\n",
    "Handles longer contexts\n",
    "\n",
    "Good for similarity search\n",
    "\n",
    "Works with FAISS / Chroma / Milvus\n",
    "\n",
    "4Ô∏è‚É£ Light / Fast Option\n",
    "HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "Embedding size: 384 (smaller)\n",
    "\n",
    "Fast, but slightly less semantic precision\n",
    "\n",
    "üéØ Recommendation for 15‚Äì50 page research papers\n",
    "\n",
    "Best Accuracy: all-mpnet-base-v2 ‚úÖ\n",
    "\n",
    "Fast + Good Accuracy: all-MiniLM-L6-v2\n",
    "\n",
    "Your current choice (all-mpnet-base-v2) is perfect for Attention Is All You Need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ddbb54-1d31-4919-b744-781c8e8d0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1:8b\",gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ad3f0f-8bcd-49ae-b592-fea34f5e9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\weare\\ansel\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.49)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\weare\\ansel\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (2.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\weare\\ansel\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\weare\\ansel\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a43e63-efec-4232-83f2-a5e92339dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88aee93d-6e9b-41d5-8871-cca9b5596e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 1.1.0\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://docs.langchain.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\weare\\ansel\\Lib\\site-packages\n",
      "Requires: langchain-core, langgraph, pydantic\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e9445be-9d31-48da-9c71-79130c3b4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\weare\\ansel\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.49)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\weare\\ansel\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (2.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\weare\\ansel\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\weare\\ansel\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\weare\\ansel\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\weare\\ansel\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\weare\\ansel\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4452a5d-54b2-4f57-af5f-f19b05b19289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2603267-54d3-47df-b3ee-b53c56ee64f2",
   "metadata": {},
   "source": [
    "- This creates a custom prompt template for a Retrieval-Augmented Generation (RAG) chatbot using LangChain.\n",
    "\n",
    "\n",
    "- PromptTemplate: A LangChain utility to define dynamic prompts with placeholders.\n",
    "- input_variables: These are the dynamic fields (chat_history, context, question) that will be filled at runtime.\n",
    "- template: The actual prompt structure. It guides the LLM to:\n",
    "- Read the chat history (for continuity),\n",
    "- Use the retrieved context (from vector store),\n",
    "- Answer the latest user question.\n",
    "\n",
    "‚úÖ Use Case\n",
    "This prompt is ideal for chatbots with memory + retrieval, where:\n",
    "- chat_history maintains conversation flow,\n",
    "- context comes from relevant document chunks,\n",
    "- question is the current user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151366b-904e-4183-aa32-0751928e774d",
   "metadata": {},
   "source": [
    "Explanation (in short):\n",
    "Takes a list of documents (docs).\n",
    "Extracts the text (page_content) from each document.\n",
    "Joins all the text together with two new lines (\\n\\n) between them.\n",
    "Returns one clean combined string.\n",
    "So it basically merges multiple document texts into one formatted text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71436ecb-7cc2-436f-95e2-f85fe4baaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1efabc5-c604-4e4f-9080-6d13d06c9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    docs = db.similarity_search(query, k=5)       # 1. Find top 5 most relevant chunks from the vector DB\n",
    "    context = format_docs(docs)                   # 2. Convert those chunks into a single text block\n",
    "\n",
    "    history_text = \"\\n\".join(chat_history)        # 3. Convert chat history list ‚Üí single string\n",
    "\n",
    "    final_prompt = prompt.format(                 # 4. Fill your RAG prompt template with:\n",
    "        chat_history=history_text,\n",
    "        context=context,\n",
    "        question=query\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(final_prompt)           # 5. Ask the LLM using the combined prompt\n",
    "\n",
    "    chat_history.append(f\"User: {query}\")         # 6. Add new user message to history\n",
    "    chat_history.append(f\"AI: {response}\")        # 7. Add AI reply to history\n",
    "\n",
    "    return response                               # 8. Return answer to user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74134b51-85e7-4a0c-a7c8-0fb2da206210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The self-attention mechanism is a crucial component of the Transformer model, allowing it to attend to different parts of the input sequence and weigh their importance for generating the output. It's a way for the model to understand relationships between different tokens or positions in the input.\n",
      "\n",
      "In simple terms, self-attention works by computing a weighted sum of the values from different positions in the input, where the weights are learned during training. The input is split into three components: queries (Q), keys (K), and values (V). The model then computes attention scores between each query and key, which represents how relevant each pair is to each other.\n",
      "\n",
      "The attention mechanism consists of two main steps:\n",
      "\n",
      "1. **Attention calculation**: The model computes the attention weights (Œ±) by taking the dot product of Q and K, and applying a softmax function to get the normalized weights.\n",
      "2. **Weighted sum**: The model takes the weighted sum of the values V, using the attention weights Œ±.\n",
      "\n",
      "The output is then computed as a weighted sum of the values from different positions in the input, where the weights are learned during training.\n",
      "\n",
      "In the context of the provided figures (Figure 3, Figure 4, and Figure 5), self-attention helps the model understand long-distance dependencies between tokens. For example, in Figure 3, the attention mechanism is able to attend to a distant dependency of the verb \"making\" in layer 5 of 6.\n",
      "\n",
      "Self-attention has several advantages over traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs):\n",
      "\n",
      "1. **Parallelization**: Self-attention allows for parallel processing of different parts of the input sequence, making it more efficient.\n",
      "2. **Scalability**: Self-attention can handle longer sequences than RNNs or CNNs, which are limited by their sequential nature.\n",
      "3. **Interpretability**: The attention weights provide insights into which parts of the input are relevant to the output.\n",
      "\n",
      "The sinusoidal positional encoding (Equation 1) is used in the self-attention mechanism to encode the position information. This allows the model to easily learn to attend by relative positions, as mentioned in the paper.\n",
      "\n",
      "I hope this explanation helps you understand the self-attention mechanism!\n",
      "In this case, the question is already answered in detail. However, I will provide a concise summary and highlight the key points.\n",
      "\n",
      "**Positional Encoding**\n",
      "\n",
      "The positional encoding is used to inject information about the relative or absolute position of tokens in a sequence into the model. This is necessary because self-attention mechanisms do not have a natural notion of order or position.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "The positional encoding uses sine and cosine functions of different frequencies to represent each dimension of the input embeddings. The formula for positional encoding is:\n",
      "\n",
      "P E(pos,2i) = sin(pos/100002i/dmodel )\n",
      "P E(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      "\n",
      "Where `pos` is the position, and `i` is the dimension.\n",
      "\n",
      "**Key Points**\n",
      "\n",
      "* Positional encoding is used to provide information about token positions in a sequence.\n",
      "* It uses sine and cosine functions of different frequencies to represent each dimension.\n",
      "* The positional encodings have the same dimension as the embeddings (dmodel), so they can be summed together.\n",
      "\n",
      "Let me know if you'd like any further clarification!\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Explain self-attention mechanism\"\n",
    "answer1 = ask_question(query1)\n",
    "print(answer1)\n",
    "\n",
    "query2 = \"How does positional encoding work?\"\n",
    "answer2 = ask_question(query2)\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef9399-b3d7-4336-bd06-f83212d4a63b",
   "metadata": {},
   "source": [
    "#### Sends the question ‚ÄúExplain self-attention mechanism‚Äù to your ask_question() function.\n",
    "\n",
    "-  RAG system retrieves relevant chunks ‚Üí builds prompt ‚Üí gets LLM answer.\n",
    "\n",
    "- Stores the answer in answer1.\n",
    "\n",
    "- Prints the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dbb9829-9a28-49d5-9393-44ab698cf46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this case, the question is already answered in detail. However, I will provide a concise summary and highlight the key points.\n",
      "\n",
      "**Positional Encoding**\n",
      "\n",
      "The positional encoding is used to inject information about the relative or absolute position of tokens in a sequence into the model. This is necessary because self-attention mechanisms do not have a natural notion of order or position.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "The positional encoding uses sine and cosine functions of different frequencies to represent each dimension of the input embeddings. The formula for positional encoding is:\n",
      "\n",
      "P E(pos,2i) = sin(pos/100002i/dmodel )\n",
      "P E(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      "\n",
      "Where `pos` is the position, and `i` is the dimension.\n",
      "\n",
      "**Key Points**\n",
      "\n",
      "* Positional encoding is used to provide information about token positions in a sequence.\n",
      "* It uses sine and cosine functions of different frequencies to represent each dimension.\n",
      "* The positional encodings have the same dimension as the embeddings (dmodel), so they can be summed together.\n",
      "\n",
      "Let me know if you'd like any further clarification!\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Encoder and Decoder Stacks?\"\n",
    "answer3 = ask_question(query2)\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5505b67e-dd74-4c01-8106-e70f4ade3ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems we've reached the end of our conversation! To summarize, we discussed self-attention mechanisms and their application in natural language processing. We covered the following topics:\n",
      "\n",
      "1. **Self-Attention Mechanism**: Self-attention is a crucial component of the Transformer model that allows it to attend to different parts of the input sequence and weigh their importance for generating the output.\n",
      "2. **Positional Encoding**: Positional encoding is used to inject information about the relative or absolute position of tokens in a sequence into the model. This is necessary because self-attention mechanisms do not have a natural notion of order or position.\n",
      "3. **Why Self-Attention**: Self-attention was used in this particular work because it addresses three key desiderata for mapping one variable-length sequence to another: parallelization, scalability, and interpretability.\n",
      "\n",
      "We also reviewed some figures and papers related to self-attention mechanisms and neural machine translation.\n",
      "\n",
      "If you have any further questions or need additional clarification on any of these topics, feel free to ask!\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, disc\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Min\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\n",
      "translation system: Bridging the gap between human and machine translation. \n"
     ]
    }
   ],
   "source": [
    "query4 = \"Conclusion\"\n",
    "answer4 = ask_question(query4)\n",
    "print(answer4)\n",
    "docs = retriever.invoke(question)\n",
    "for d in docs:\n",
    "    print(d.page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bad5dd3-37fc-4a4d-88f0-ee62680d65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the PDF are not explicitly stated in the provided snippet, but based on the references and citations mentioned, it appears to be a collection of research papers and articles related to natural language processing and neural machine translation.\n",
      "\n",
      "However, I can provide some information about the specific papers cited:\n",
      "\n",
      "* The paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al., 2014) is attributed to Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\n",
      "* The paper \"Massive Exploration of Neural Machine Translation Architectures\" (Britz et al., 2017) is attributed to Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le.\n",
      "\n",
      "If you're looking for the authors of the original PDF, I'd be happy to help you investigate further!\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, disc\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Min\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\n",
      "translation system: Bridging the gap between human and machine translation. \n"
     ]
    }
   ],
   "source": [
    "query4 = \"author of pdf \"\n",
    "answer4 = ask_question(query4)\n",
    "print(answer4)\n",
    "docs = retriever.invoke(question)\n",
    "for d in docs:\n",
    "    print(d.page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fd3c8bc-67f7-4621-bc57-0074793d80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scaled Dot-Product Attention is a type of attention mechanism used in the Transformer model. It works by computing the dot products of the query with all keys, dividing each by ‚àödk, and applying a softmax function to obtain the weights on the values.\n",
      "\n",
      "The formula for Scaled Dot-Product Attention is given by:\n",
      "\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "‚àödk\n",
      ")V\n",
      "\n",
      "Where Q is the matrix of queries, K is the matrix of keys, V is the matrix of values, and dk is the dimension of the keys. The dot products are scaled by ‚àödk to prevent the dot products from growing too large in magnitude.\n",
      "\n",
      "The Scaled Dot-Product Attention has several advantages over other attention mechanisms, including additive attention, such as:\n",
      "\n",
      "* It can be implemented using highly optimized matrix multiplication code.\n",
      "* It is much faster and more space-efficient than additive attention.\n",
      "* It outperforms additive attention for small values of dk.\n",
      "\n",
      "However, for larger values of dk, the dot products can grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, scaling the dot products by 1‚àödk is used.\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, disc\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Min\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\n",
      "translation system: Bridging the gap between human and machine translation. \n"
     ]
    }
   ],
   "source": [
    "query4 = \"Scaled Dot-Product Attention\"\n",
    "answer4 = ask_question(query4)\n",
    "print(answer4)\n",
    "docs = retriever.invoke(question)\n",
    "for d in docs:\n",
    "    print(d.page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb250b-7052-48f4-8b2d-da87ca3da057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
